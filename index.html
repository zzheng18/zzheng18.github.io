<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Jimuyang Zhang</title>

  <meta name="author" content="Jimuyang Zhang">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="google-site-verification" content="6qogr8z7RZgR_SCX2GSXZNaoOxokGl59tYYreLe6uEg" />
  <meta property="og:image" content="images_jim/jimuyang.png" />
  <meta name="og:title" content="Jimuyang Zhang" />
  <meta name="title" content="Jimuyang Zhang" />
  <link rel=" stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon"
    href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>


<body>
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p style="text-align:center">
                    <name>Jimuyang Zhang</name>
                  <p>I am a third-year PhD student at Boston University, advised by Prof. <a
                      href="https://eshed1.github.io/">Eshed Ohn-Bar</a>.
                    
                  <p>
                    Prior to BU, I worked with Dr. <a href="https://www.donghuang-research.com/">Dong Huang</a> as a research assistant at the <a href="http://humansensing.cs.cmu.edu/">Human
                    Sensing Lab</a>. I got my master's degree (2016-2018) at the Robotics Institute of Carnegie Mellon University, where I worked with Prof. <a href="http://www.cs.cmu.edu/~kkitani/">Kris
                      Kitani</a>, on pedestrian detection on low-profile robot.


                  <p style="text-align:center">
                    <a href="mailto:zhangjimuyang@gmail.com">Email</a> &nbsp/&nbsp
                    <a href="https://github.com/Jimuyangz/">Github</a> &nbsp/&nbsp
                    <a href="https://scholar.google.com/citations?user=9spN7eUAAAAJ&hl=en">Google Scholar</a>
                  </p>
                </td>
                <td style="padding:2.5%;width:40%;max-width:40%">
                  <a href="images_jim/jimuyang_circle.png"><img style="width:100%;max-width:100%" alt="profile photo"
                      src="images_jim/jimuyang_circle.png" class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>




          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Research</heading>
                  <p>My research interests lie in computer vision, robotics and machine learning with their applications in autonomous and assistive systems.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              
              
              
              
              <tr onmouseout="eccv_2022_stop()" onmouseover="eccv_2022_start()">
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <div class="one">
                    <div id='eccv_2022_still'><img src='images_jim/simulation.PNG' width=100%></div>
                  </div>
                  <script type="text/javascript">
                    function eccv_2022_start() {
                      document.getElementById('eccv_2022_still').style.display = 'none';
                    }

                    function eccv_2022_stop() {
                      document.getElementById('eccv_2022_still').style.display = 'inline';
                    }
                    eccv_2022_stop()
                  </script>
                </td>



                <td
                  style="padding-left:20px;padding-right: 20px; padding-top:10px;padding-bottom: 10px;width:75%;vertical-align:middle">
                  <a href="https://eshed1.github.io/papers/assister_eccv2022.pdf">
                    <papertitle>ASSISTER: Assistive Navigation via Conditional Instruction Generation
                      </papertitle>
                  </a>
                  <br>
                  Zanming Huang*, Zhongkai Shangguan*, <strong>Jimuyang Zhang</strong>, Gilad Bar, <a href="https://www.linkedin.com/in/mattcboyd/">Matthew Boyd</a>,
                   <a href="https://scholar.google.com/citations?user=p9zVBV4AAAAJ&hl=en&oi=ao">Eshed Ohn-Bar</a>
                  <br>
                  <em>European Conference on Computer Vision (ECCV)</em>, 2022
                  <br>
                  <a href="https://eshed1.github.io/papers/assister_eccv2022.pdf">paper</a> &nbsp/&nbsp
                  <a href="https://github.com/h2xlab/ASSISTER">data and code</a>
                  <p></p>
                  <p>We introduce a novel vision-and-language navigation (VLN) task of learning to provide real-time guidance to a blind follower situated 
                    in complex dynamic navigation scenarios. We collect a multi-modal real-world benchmark with in-situ Orientation and Mobility (O&M) 
                    instructional guidance. We leverage the real-world study to inform the design of a larger-scale simulation benchmark. In the end, we present ASSISTER, an imitation-learned agent that can 
                    embody such effective guidance.
                  </p>
                </td>
              </tr>
              <!-- END PAPER Assister -->
              
              
              


              <tr onmouseout="cvpr_2022_stop()" onmouseover="cvpr_2022_start()">
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='cvpr_2022_video'><video width=100% muted autoplay loop>
                        <source src="images_jim/selfd_video.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                      </video></div>
                    <div id='cvpr_2022_still'><img src='images_jim/selfd_f1.png' width=100%></div>
                  </div>
                  <script type="text/javascript">
                    function cvpr_2022_start() {
                      document.getElementById('cvpr_2022_video').style.display = 'inline';
                      document.getElementById('cvpr_2022_still').style.display = 'none';
                    }

                    function cvpr_2022_stop() {
                      document.getElementById('cvpr_2022_video').style.display = 'none';
                      document.getElementById('cvpr_2022_still').style.display = 'inline';
                    }
                    cvpr_2022_stop()
                  </script>
                </td>



                <td
                  style="padding-left:20px;padding-right: 20px; padding-top:10px;padding-bottom: 10px;width:75%;vertical-align:middle">
                  <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zhang_SelfD_Self-Learning_Large-Scale_Driving_Policies_From_the_Web_CVPR_2022_paper.pdf">
                    <papertitle>SelfD: Self-Learning Large-Scale Driving Policies From the Web
                      </papertitle>
                  </a>
                  <br>
                  <strong>Jimuyang Zhang</strong>,
                  <a href="https://scholar.google.com/citations?user=otVAkGkAAAAJ&hl=en">Ruizhao Zhu</a>,
                  <a href="https://scholar.google.com/citations?user=p9zVBV4AAAAJ&hl=en&oi=ao">Eshed Ohn-Bar</a>
                  <br>
                  <em>Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2022
                  <br>
                  <a href="https://arxiv.org/pdf/2204.10320.pdf">arxiv</a> &nbsp/&nbsp
                  <a href="https://www.youtube.com/watch?v=nWhdxx_9o58">video</a>
                  <p></p>
                  <p>We introduce SelfD, a framework for learning scalable driving by utilizing large amounts of online monocular images. Our key idea is to leverage iterative semi-supervised training when learning imitative agents from unlabeled data. We employ a large dataset of publicly available YouTube videos to train SelfD and comprehensively analyze its generalization benefits across challenging navigation scenarios. SelfD demonstrates consistent improvements (by up to 24%) in driving performance evaluation on nuScenes, Argoverse, Waymo, and CARLA.
                  </p>
                </td>
              </tr>
              <!-- END PAPER Selfd -->



            <tr onmouseout="iccv_2021_stop()" onmouseover="iccv_2021_start()">
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='iccv_2021_video'><video width=100% muted autoplay loop>
                        <source src="images_jim/xworld_video.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                      </video></div>
                    <div id='iccv_2021_still'><img src='images_jim/xworld.png' width=100%></div>
                  </div>
                  <script type="text/javascript">
                    function iccv_2021_start() {
                      document.getElementById('iccv_2021_video').style.display = 'inline';
                      document.getElementById('iccv_2021_still').style.display = 'none';
                    }

                    function iccv_2021_stop() {
                      document.getElementById('iccv_2021_video').style.display = 'none';
                      document.getElementById('iccv_2021_still').style.display = 'inline';
                    }
                    iccv_2021_stop()
                  </script>
                </td>



                <td
                  style="padding-left:20px;padding-right: 20px; padding-top:10px;padding-bottom: 10px;width:75%;vertical-align:middle">
                  <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Zhang_X-World_Accessibility_Vision_and_Autonomy_Meet_ICCV_2021_paper.pdf">
                    <papertitle>X-World: Accessibility, Vision, and Autonomy Meet</papertitle>
                  </a>
                  <br>
                  <strong>Jimuyang Zhang*</strong>,
                  <a href="https://www.linkedin.com/in/minglan-zheng-611b59174/">Minglan Zheng*</a>,
                  <a href="https://www.linkedin.com/in/mattcboyd/">Matthew Boyd</a>,
                  <a href="https://scholar.google.com/citations?user=p9zVBV4AAAAJ&hl=en&oi=ao">Eshed Ohn-Bar</a>
                  <br>
                  <em>International Conference on Computer Vision (ICCV)</em>, 2021
                  <br>
                  <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Zhang_X-World_Accessibility_Vision_and_Autonomy_Meet_ICCV_2021_paper.pdf">arxiv</a> &nbsp/&nbsp
                  <a href="https://www.youtube.com/watch?v=z_YwWIZWg58">video</a> &nbsp/&nbsp
                  <a href="https://www.youtube.com/watch?v=9EiX4BxC2b8">talk</a> &nbsp/&nbsp
                  <a href="https://accessibility-cv.github.io/">workshop</a> &nbsp/&nbsp
                  <a href="https://eval.ai/web/challenges/challenge-page/1690/overview">challenge</a>
                  <p></p>
                  <p>We introduce X-World, an accessibility-centered development environment for vision-based autonomous systems, which enables spawning dynamic agents with various mobility aids. The simulation supports generation of ample amounts of finely annotated, multi-modal data in a safe, cheap, and privacy-preserving manner. We highlight novel difficulties introduced by our benchmark and tasks, as well as opportunities for future developments. We further validate and extend our analysis by introducing a complementary real-world evaluation benchmark.
                  </p>
                </td>
              </tr>
              <!-- END PAPER xworld -->






              <tr onmouseout="cvpr_2021_stop()" onmouseover="cvpr_2021_start()">
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='cvpr_2021_video'><video width=100% muted autoplay loop>
                        <source src="images_jim/lbw_video.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                      </video></div>
                    <div id='cvpr_2021_still'><img src='images_jim/lbw.png' width=100%></div>
                  </div>
                  <script type="text/javascript">
                    function cvpr_2021_start() {
                      document.getElementById('cvpr_2021_video').style.display = 'inline';
                      document.getElementById('cvpr_2021_still').style.display = 'none';
                    }

                    function cvpr_2021_stop() {
                      document.getElementById('cvpr_2021_video').style.display = 'none';
                      document.getElementById('cvpr_2021_still').style.display = 'inline';
                    }
                    cvpr_2021_stop()
                  </script>
                </td>



                <td
                  style="padding-left:20px;padding-right: 20px; padding-top:10px;padding-bottom: 10px;width:75%;vertical-align:middle">
                  <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_Learning_by_Watching_CVPR_2021_paper.pdf">
                    <papertitle>Learning by Watching</papertitle>
                  </a>
                  <br>
                  <strong>Jimuyang Zhang</strong>,
                  <a href="https://scholar.google.com/citations?user=p9zVBV4AAAAJ&hl=en&oi=ao">Eshed Ohn-Bar</a>
                  <br>
                  <em>Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2021
                  <br>
                  <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_Learning_by_Watching_CVPR_2021_paper.pdf">arxiv</a> &nbsp/&nbsp
                  <a href="https://www.youtube.com/watch?v=ApyIcoTDyc8">video</a>
                  <p></p>
                  <p>We propose the Learning by Watching (LbW) framework which enables learning a driving policy without requiring full knowledge of neither the state nor expert actions. To increase its data, i.e., with new perspectives and maneuvers, LbW makes use of the demonstrations of other vehicles in a given scene by (1) transforming the egovehicle‚Äôs observations to their points of view, and (2) inferring their expert actions. Our LbW agent learns more robust driving policies while enabling data-efficient learning, including quick adaptation of the policy to rare and novel scenarios.
                  </p>
                </td>
              </tr>
              <!-- END PAPER lbw -->
              
              
              
              
              <td style="padding:20px;width:30%;vertical-align:middle">
                  <div class="one">
                    <div id='tracking'><img src='images_jim/tracking.png' width=100%></div>
                  </div>
                </td>



                <td
                  style="padding-left:20px;padding-right: 20px; padding-top:10px;padding-bottom: 10px;width:75%;vertical-align:middle">
                  <a href="https://arxiv.org/pdf/2001.11180.pdf">
                    <papertitle>Multiple Object Tracking by Flowing and Fusing</papertitle>
                  </a>
                  <br>
                  <strong>Jimuyang Zhang</strong>,
                  <a href="https://scholar.google.com/citations?user=2Drvv44AAAAJ&hl=zh-CN">Sanping Zhou</a>,
                  Xin Chang, Fangbin Wan,
                  <a href="https://scholar.google.com/citations?user=Dk7JgNcAAAAJ&hl=zh-CN">Jinjun Wang</a>,
                  <a href="https://scholar.google.com.hk/citations?user=vwOQ-UIAAAAJ&hl=zh-CN">Yang Wu</a>,
                  <a href="https://scholar.google.com/citations?user=VeXMKBoAAAAJ&hl=zh-CN">Dong Huang</a>
                  <br>
                  <em>arXiv</em>, 2020
                  <br>
                  <a href="https://arxiv.org/pdf/2001.11180.pdf">arxiv</a>
                  <p></p>
                  <p>We design an end-to-end DNN tracking approach, Flow-Fuse-Tracker (FFT), consisting of two efficient techniques: target flowing and target fusing. In target flowing, a FlowTracker DNN module learns the indefinite number of target-wise motions jointly from pixel-level optical flows. In target fusing, a FuseTracker DNN module refines and fuses targets proposed by FlowTracker and frame-wise object detection, instead of trusting either of the two inaccurate sources of target proposal. 
                  </p>
                </td>
              </tr>
              <!-- END PAPER tracking -->











            </tbody>
          </table>








          <!-- Teaching -->

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Teaching</heading>

                  <p>
                    <a href="https://www.bu.edu/academics/eng/courses/eng-ec-444/">ENG EC 444 - Smart and Connected Systems - Fall 2022</a>
                    <br>
                    Teaching Assistant
                  </p>

                  <p>
                    <a href="https://www.bu.edu/academics/eng/courses/eng-ec-503/">ENG EC 503 - Introduction to Learning from Data - Fall 2021</a>
                    <br>
                    Teaching Assistant
                  </p>

                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>


            </tbody>
          </table>



          <!-- Service -->

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Service</heading>

                  <p>
                    <a href="https://accessibility-cv.github.io/">CVPR2022 AVA Accessibility Vision and Autonomy Challenge</a>
                    <br>
                    Challenge Organizer
                  </p>

                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>


            </tbody>
          </table>










          <tables
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px">
                  <br>
                  <p style="text-align:right;font-size:small;">
                    This website was forked from <a href="https://github.com/jonbarron/jonbarron_website">source
                      code</a>

                  </p>
                </td>
              </tr>
            </tbody>
          </table>
        </td>
      </tr>
  </table>
</body>

</html>




